import json
import gradio as gr
import torch
import os
import gc
import traceback
from transformers import AutoModelForCausalLM, AutoTokenizer

from vision import VisionModule
from agent import ToolUseAgent
from tools import TOOLS_SCHEMA

# --- Cáº¥u hÃ¬nh ---
MODEL_OPTIONS = {
    "Qwen Agent": "src_agent/agent_model_weights/checkpoint-318",
    "Vietnamse Qwen 2.5 Math (1.5B)": "piikerpham/Vietnamese-Qwen2.5-math-1.5B",
    "Qwen 2.5 Math (1.5B)": "Qwen/Qwen2.5-Math-1.5B-Instruct"
    # "Qwen 2.5 Math (7B)": "Qwen/Qwen2.5-Math-7B-Instruct", # Báº£n 7B cÃ³ thá»ƒ quÃ¡ náº·ng náº¿u muá»‘n kiá»ƒm tra thÃ¬ má»›i thÃªm vÃ o
}

# --- Biáº¿n ToÃ n cá»¥c ---
current_model = None
current_tokenizer = None
current_agent = None
loaded_model_name = ""
vision_module = VisionModule()

# --- XÃ¡c Ä‘á»‹nh thiáº¿t bá»‹ (Device) phÃ¹ há»£p ---
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("ÄÃ£ phÃ¡t hiá»‡n GPU Apple (MPS). Model sáº½ Ä‘Æ°á»£c tÄƒng tá»‘c.")
elif torch.cuda.is_available():
    # Giá»¯ láº¡i Ä‘á»ƒ code váº«n cháº¡y Ä‘Æ°á»£c trÃªn mÃ¡y cÃ³ card NVIDIA
    device = torch.device("cuda")
    print("ÄÃ£ phÃ¡t hiá»‡n GPU NVIDIA (CUDA).")
else:
    device = torch.device("cpu")
    print("KhÃ´ng phÃ¡t hiá»‡n GPU tÆ°Æ¡ng thÃ­ch, Ä‘ang sá»­ dá»¥ng CPU. Tá»‘c Ä‘á»™ sáº½ cháº­m.")


def clean_memory():
    """HÃ m dá»n dáº¹p bá»™ nhá»›, tÆ°Æ¡ng thÃ­ch vá»›i nhiá»u ná»n táº£ng."""
    global current_model, current_tokenizer, current_agent

    del current_model
    del current_tokenizer
    del current_agent

    current_model = None
    current_tokenizer = None
    current_agent = None

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()


    gc.collect()
    print("ÄÃ£ dá»n dáº¹p bá»™ nhá»›.")


def load_model_pipeline(model_key):
    """HÃ m load model vÃ  tokenizer, Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u cho Mac."""
    global current_model, current_tokenizer, current_agent, loaded_model_name, device

    if loaded_model_name == model_key and current_agent is not None:
        return f"Model '{model_key}' Ä‘Ã£ sáºµn sÃ ng!"

    print(f"Äang chuyá»ƒn Ä‘á»•i sang model: {model_key}...")

    # Dá»n dáº¹p model cÅ© trÆ°á»›c khi load model má»›i
    if current_model is not None or current_agent is not None:
        clean_memory()

    model_path = MODEL_OPTIONS[model_key]
    try:
        print(f"Äang táº£i model tá»«: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)


        print("Loading model vá»›i torch_dtype=torch.float16 Ä‘á»ƒ tá»‘i Æ°u bá»™ nhá»› trÃªn Mac.")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )

        model.to(device)

        current_model = model
        current_tokenizer = tokenizer
        current_agent = ToolUseAgent(model, tokenizer, tools_metadata=TOOLS_SCHEMA)
        loaded_model_name = model_key

        print(f"Load thÃ nh cÃ´ng: {model_key} trÃªn thiáº¿t bá»‹ {device}")
        return f"ÄÃ£ chuyá»ƒn sang: {model_key}"

    except Exception as e:
        print(f"Lá»—i load model: {e}")
        traceback.print_exc()
        return f"Lá»—i: {str(e)}"


def solve_math_problem(model_select, question, image_path, show_reasoning, temperature, max_tokens):
    global current_agent, loaded_model_name, vision_module, current_model

    reasoning_display = ""
    full_question = question

    # --- Xá»­ lÃ½ áº£nh (náº¿u cÃ³) ---
    if image_path is not None:
        if current_model is not None:
            print("PhÃ¡t hiá»‡n áº£nh, táº¡m thá»i unload Math Model Ä‘á»ƒ giáº£i phÃ³ng bá»™ nhá»› cho Vision Model...")
            clean_memory()

        reasoning_display += "###Xá»­ lÃ½ HÃ¬nh áº£nh (Vintern-1B)\n"
        try:
            extracted_text = vision_module.extract_text_from_image(image_path)
            reasoning_display += f"> **Ná»™i dung trÃ­ch xuáº¥t:**\n{extracted_text}\n\n---\n"
            full_question = f"{extracted_text}\n\n{question}"
        except Exception as e:
            reasoning_display += f"> Lá»—i Ä‘á»c áº£nh: {str(e)}\n\n---\n"


    if current_agent is None or loaded_model_name != model_select:
        status = load_model_pipeline(model_select)
        if "Lá»—i" in status:
            return status, reasoning_display

    if not current_agent:
        return "Lá»—i: KhÃ´ng thá»ƒ khá»Ÿi táº¡o Agent.", reasoning_display
    if not full_question.strip():
        return "Vui lÃ²ng nháº­p cÃ¢u há»i hoáº·c upload áº£nh.", reasoning_display

    current_agent.generation_cfg = {
        "max_new_tokens": max_tokens,
        "temperature": temperature,
        "do_sample": True if temperature > 0 else False,
    }

    try:
        print(f"Agent Ä‘ang suy luáº­n vá»›i model: {loaded_model_name} trÃªn thiáº¿t bá»‹ {current_model.device}")
        conversations, final_answer = current_agent.inference(full_question)


        if show_reasoning:
            step_count = 1
            for msg in conversations:
                role = msg['role']
                content = str(msg['content'])
                if role == 'assistant':
                    if "<tool_call>" in content:
                        parts = content.split("<tool_call>")
                        thought = parts[0].strip()
                        tool_code = parts[1].replace("</tool_call>", "").strip()
                        reasoning_display += f"### BÆ°á»›c {step_count}: Suy luáº­n\n"
                        if thought: reasoning_display += f"{thought}\n\n"
                        reasoning_display += f"**âš¡ HÃ nh Ä‘á»™ng:**\n```json\n{tool_code}\n```\n\n"
                        step_count += 1
                    else:
                        if content.strip() != final_answer.strip():
                            reasoning_display += f"###  BÆ°á»›c {step_count}: Suy luáº­n\n{content}\n\n"
                            step_count += 1
                elif role == 'tool':
                    clean_res = content.replace("<tool_response>", "").replace("</tool_response>", "").strip()
                    reasoning_display += f"### ðŸ”§ Káº¿t quáº£ CÃ´ng cá»¥\n> {clean_res}\n\n---\n"

        if not final_answer:
            final_answer = conversations[-1]['content']

        return final_answer, reasoning_display

    except Exception as e:
        traceback.print_exc()
        return f"Lá»—i há»‡ thá»‘ng: {str(e)}", reasoning_display

# --- GRADIO UI ---
css = """
#reasoning_box { background-color: #f9f9f9; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px; max-height: 500px; overflow-y: auto; }
#status_box { font-weight: bold; color: #2e7d32; }
"""
with gr.Blocks(title="Math Agent + Vintern Vision", theme=gr.themes.Soft(), css=css) as demo:
    gr.Markdown("# Há»‡ thá»‘ng Giáº£i ToÃ¡n Äa PhÆ°Æ¡ng Thá»©c (Vintern + Qwen)")
    with gr.Row():
        with gr.Column(scale=4):
            with gr.Group():
                gr.Markdown("### 1. Cáº¥u hÃ¬nh Model")
                model_selector = gr.Dropdown(
                    choices=list(MODEL_OPTIONS.keys()),
                    value="Vietnamse Qwen 2.5 Math (1.5B)",
                    label="Math Agent Model",
                    interactive=True
                )
                load_status = gr.Textbox(label="Tráº¡ng thÃ¡i", value="Khá»Ÿi Ä‘á»™ng...", elem_id="status_box", interactive=False)
            with gr.Group():
                gr.Markdown("### 2. Nháº­p Äá» BÃ i")
                image_input = gr.Image(type="filepath", label="Upload áº£nh bÃ i toÃ¡n")
                question_input = gr.Textbox(lines=3, placeholder="Nháº­p thÃªm yÃªu cáº§u (VD: Giáº£i chi tiáº¿t bÃ i toÃ¡n trÃªn)...", label="CÃ¢u há»i bá»• sung")
            with gr.Accordion("Cáº¥u hÃ¬nh nÃ¢ng cao", open=False):
                temperature = gr.Slider(0.0, 1.0, 0.5, label="Temperature")
                max_tokens = gr.Slider(128, 2048, 1024, label="Max Tokens")
                show_reasoning = gr.Checkbox(True, label="Hiá»‡n suy luáº­n")
            solve_btn = gr.Button("GIáº¢I BÃ€I NGAY", variant="primary", size="lg")
        with gr.Column(scale=5):
            gr.Markdown("### Káº¿t quáº£ cuá»‘i cÃ¹ng")
            answer_output = gr.Textbox(label="", interactive=False, lines=3)
            gr.Markdown("### QuÃ¡ trÃ¬nh suy luáº­n (Vision -> Thought -> Tools)")
            reasoning_output = gr.Markdown(elem_id="reasoning_box")

    model_selector.change(fn=load_model_pipeline, inputs=[model_selector], outputs=[load_status])
    solve_btn.click(fn=solve_math_problem, inputs=[model_selector, question_input, image_input, show_reasoning, temperature, max_tokens], outputs=[answer_output, reasoning_output])

    # Tá»± Ä‘á»™ng load model máº·c Ä‘á»‹nh khi khá»Ÿi Ä‘á»™ng app
    demo.load(fn=load_model_pipeline, inputs=[model_selector], outputs=[load_status])

if __name__ == "__main__":
    demo.launch(share=True)