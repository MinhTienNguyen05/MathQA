import torch
import json
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForLanguageModeling
from utils import parse_tool_call_from_text
from tools import WikipediaRetriever, evaluate, solve_equation, convert_units
from peft import LoraConfig, get_peft_model, TaskType

DEFAULT_SYSTEM_PROMPT = """B·∫°n l√† m·ªôt chuy√™n gia to√°n h·ªçc v√† l·∫≠p tr√¨nh si√™u vi·ªát. Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi·∫£i quy·∫øt c√°c b√†i to√°n ph·ª©c t·∫°p b·∫±ng c√°ch s·ª≠ d·ª•ng c√¥ng c·ª• (Tools) m·ªôt c√°ch ch√≠nh x√°c.

QUY T·∫ÆC B·∫§T DI B·∫§T D·ªäCH (B·∫ÆT BU·ªòC TU√ÇN TH·ª¶):
1. üö´ KH√îNG BAO GI·ªú D·ª™NG L·∫†I khi ch·ªâ m·ªõi n√™u k·∫ø ho·∫°ch (V√≠ d·ª•: "T√¥i s·∫Ω t√≠nh...", "ƒê·∫ßu ti√™n..."). 
2. ‚ö° H√ÄNH ƒê·ªòNG NGAY: Ngay sau khi suy nghƒ©, b·∫°n PH·∫¢I vi·∫øt code g·ªçi tool (ƒë·ªãnh d·∫°ng JSON) ho·∫∑c ƒë∆∞a ra ph√©p t√≠nh ngay l·∫≠p t·ª©c.
3. üõ† S·ª¨ D·ª§NG TOOL: V·ªõi c√°c ph√©p t√≠nh ph·ª©c t·∫°p (s·ªë l·ªõn, ph∆∞∆°ng tr√¨nh, cƒÉn b·∫≠c), M·∫ÆT BU·ªòC ph·∫£i g·ªçi tool.
4. üèÅ K·∫æT LU·∫¨N: C√¢u tr·∫£ l·ªùi cu·ªëi c√πng ph·∫£i ng·∫Øn g·ªçn v√† ch·ª©a ƒë√°p √°n s·ªë h·ªçc ch√≠nh x√°c (V√≠ d·ª•: "ƒê√°p √°n l√†: 10").

ƒê·ªãnh d·∫°ng g·ªçi Tool:
<tool_call>
{"name": "t√™n_h√†m", "arguments": {"arg1": "gi√°_tr·ªã"}}
</tool_call>
"""

class ToolUseAgent:
    def __init__(self, model, tokenizer, tools_metadata=None, system_prompt=None, generation_cfg=None):
        self.model = model
        self.tokenizer = tokenizer
        self.tools = tools_metadata or []
        
        self.system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
        
        self.generation_cfg = generation_cfg or {
            "max_new_tokens": 1024,   
            "do_sample": False,      
            "temperature": 0.0,      
            "repetition_penalty": 1.05 
        }

    def invoke_tool(self, tool_name, args) -> str:
        """G·ªçi h√†m t∆∞∆°ng ·ª©ng d·ª±a tr√™n t√™n tool."""
        normalized = tool_name.replace("_", "").lower()
        
        try:
            if normalized in ("wikipediaretriever", "wikipediasearch"):
                return str(WikipediaRetriever(**args))
            elif normalized in ("evaluate", "calculator", "calculate"):
                return str(evaluate(**args))
            elif normalized in ("solveequation", "solve"):
                return str(solve_equation(**args))
            elif normalized in ("convertunits", "unitconverter"):
                return str(convert_units(**args))
            else:
                return f"Error: Tool `{tool_name}` not found."
        except Exception as e:
            return f"Error executing {tool_name}: {str(e)}"

    def call_llm(self, conversations: list):
        """Sinh vƒÉn b·∫£n t·ª´ LLM."""
        prompt_text = self.tokenizer.apply_chat_template(
            conversations,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(prompt_text, return_tensors="pt").to(self.model.device)
        
        gen_kwargs = {
            **inputs,
            "max_new_tokens": self.generation_cfg.get("max_new_tokens", 1024),
            "do_sample": self.generation_cfg.get("do_sample", False),
            "temperature": self.generation_cfg.get("temperature", 0.0),
            "repetition_penalty": self.generation_cfg.get("repetition_penalty", 1.05),
            "pad_token_id": self.tokenizer.eos_token_id
        }

        with torch.no_grad():
            outputs = self.model.generate(**gen_kwargs)
            
        generated = outputs[0, inputs["input_ids"].shape[-1] :].cpu().numpy()
        return self.tokenizer.decode(generated, skip_special_tokens=True).strip()

    def inference(self, question: str):
        """V√≤ng l·∫∑p ReAct: Suy lu·∫≠n -> G·ªçi Tool -> Nh·∫≠n k·∫øt qu·∫£ -> Tr·∫£ l·ªùi."""
        
        full_system_prompt = self.system_prompt
        if self.tools:
            tools_desc = json.dumps(self.tools, ensure_ascii=False, indent=2)
            full_system_prompt += f"\n\nDanh s√°ch c√¥ng c·ª• kh·∫£ d·ª•ng:\n{tools_desc}"

        conversations = [
            {"role": "system", "content": full_system_prompt},
            {"role": "user", "content": question},
        ]

        for _ in range(10):
            llm_response = self.call_llm(conversations)
            
            conversations.append({"role": "assistant", "content": llm_response})
            
            tool_call = parse_tool_call_from_text(llm_response)
            
            if tool_call:
                name = tool_call.get("name")
                args = tool_call.get("arguments", {})

                tool_res = self.invoke_tool(name, args)
                
                conversations.append({"role": "tool", "content": tool_res})
            else:
                break
                
        return conversations, conversations[-1]["content"]

    def train(self, train_dataset, eval_dataset, cfg):
        """Thi·∫øt l·∫≠p Trainer v√† ch·∫°y hu·∫•n luy·ªán."""
        self.model.config.use_cache = False 
        
        if getattr(cfg, "USE_LORA", False):
            print("üü¢ Setting up LoRA configuration...")
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM, 
                inference_mode=False, 
                r=cfg.LORA_R, 
                lora_alpha=cfg.LORA_ALPHA, 
                lora_dropout=cfg.LORA_DROPOUT,
                target_modules=cfg.LORA_TARGET_MODULES
            )
            self.model = get_peft_model(self.model, peft_config)
            self.model.print_trainable_parameters()
        
        training_args = TrainingArguments(
            output_dir=cfg.OUTPUT_DIR,
            num_train_epochs=cfg.EPOCHS,
            per_device_train_batch_size=cfg.BATCH_SIZE,
            per_device_eval_batch_size=getattr(cfg, 'PER_DEVICE_EVAL_BATCH_SIZE', 4),
            gradient_accumulation_steps=cfg.GRADIENT_ACCUMULATION_STEPS,
            learning_rate=cfg.LEARNING_RATE,
            logging_dir=cfg.LOGGING_DIR,
            
            # C·∫•u h√¨nh ph·∫ßn c·ª©ng (FP16/BF16)
            fp16=cfg.FP16,
            bf16=cfg.BF16,
            
            eval_strategy=cfg.EVAL_STRATEGY,
            eval_steps=cfg.EVAL_STEPS,
            save_strategy=cfg.SAVE_STRATEGY,
            save_steps=cfg.SAVE_STEPS,
            save_total_limit=cfg.SAVE_TOTAL_LIMIT,
            load_best_model_at_end=cfg.LOAD_BEST_MODEL_AT_END,
            metric_for_best_model="eval_loss",
            
            warmup_ratio=cfg.WARMUP_RATIO,
            weight_decay=cfg.WEIGHT_DECAY,
            max_grad_norm=cfg.MAX_GRAD_NORM,
            
            report_to="none",
            logging_steps=10,
            remove_unused_columns=True,
            dataloader_num_workers=2,
            
            gradient_checkpointing=True,
            gradient_checkpointing_kwargs={"use_reentrant": False},
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=self.tokenizer,
            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),
        )
        
        trainer.train()
        print(f"Saving model to {cfg.OUTPUT_DIR}...")
        trainer.save_model(cfg.OUTPUT_DIR)